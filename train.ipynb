{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../models/')\n",
    "from models.lstm import LSTM\n",
    "from models.transformer import Transformer\n",
    "from models.train_model import RPS_loss, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_home_teams_matches = th.load(\"dataset/tensors/training_home_teams_matches.pt\")\n",
    "training_away_teams_matches = th.load(\"dataset/tensors/training_away_teams_matches.pt\")\n",
    "training_matches_features_home = th.load(\"dataset/tensors/training_matches_features_home.pt\")\n",
    "training_matches_features_away = th.load(\"dataset/tensors/training_matches_features_away.pt\")\n",
    "training_targets = th.load(\"dataset/tensors/training_targets.pt\")\n",
    "\n",
    "test_home_teams_matches = th.load(\"dataset/tensors/test_home_teams_matches.pt\")\n",
    "test_away_teams_matches = th.load(\"dataset/tensors/test_away_teams_matches.pt\")\n",
    "test_matches_features_home = th.load(\"dataset/tensors/test_matches_features_home.pt\")\n",
    "test_matches_features_away = th.load(\"dataset/tensors/test_matches_features_away.pt\")\n",
    "test_targets = th.load(\"dataset/tensors/test_targets.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 0, 'D': 1, 'A': 2}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file1 = 'dataset/tensors/idx_to_teams.pkl'\n",
    "file2 = 'dataset/tensors/teams_to_idx.pkl'\n",
    "file3 = 'dataset/tensors/result_map.pkl'\n",
    "\n",
    "with open(file1, 'rb') as file:\n",
    "    idx_to_teams = pickle.load(file)\n",
    "\n",
    "with open(file2, 'rb') as file:\n",
    "    teams_to_idx = pickle.load(file)\n",
    "\n",
    "with open(file3, 'rb') as file:\n",
    "    result_map = pickle.load(file)\n",
    "\n",
    "print(result_map)\n",
    "idx_to_result = {0: 'H', 1: 'D', 2: 'A'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "torch.Size([24843])\n",
      "torch.Size([24843])\n",
      "torch.Size([24843, 5, 14])\n",
      "torch.Size([24843, 5, 14])\n",
      "torch.Size([24843])\n",
      "Test data\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 5, 14])\n",
      "torch.Size([3072, 5, 14])\n",
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\")\n",
    "print(training_home_teams_matches.shape) # the idex of home team for the matches\n",
    "print(training_away_teams_matches.shape) # the idex of away team for the matches\n",
    "print(training_matches_features_home.shape) # the features of the home team for the matches\n",
    "print(training_matches_features_away.shape) # the features of the away team for the matches\n",
    "print(training_targets.shape) # the targets of the matches H D A\n",
    "\n",
    "print(\"Test data\")\n",
    "print(test_home_teams_matches.shape)\n",
    "print(test_away_teams_matches.shape)\n",
    "print(test_matches_features_home.shape)\n",
    "print(test_matches_features_away.shape)\n",
    "print(test_targets.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24843, 3])\n",
      "torch.Size([3072, 3])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([1., 0., 0.])\n",
      "tensor([0., 1., 0.])\n",
      "tensor([0., 0., 1.])\n",
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot_targets(targets):\n",
    "    num_classes = th.max(targets).item() + 1\n",
    "    return F.one_hot(targets, num_classes=num_classes).type(th.float32)\n",
    "\n",
    "training_targets_oh = one_hot_targets(training_targets)\n",
    "test_targets_oh = one_hot_targets(test_targets)\n",
    "\n",
    "print(training_targets_oh.shape)\n",
    "print(test_targets_oh.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    print(training_targets_oh[i])\n",
    "    print(test_targets_oh[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First match\n",
      "Home team:  Bourg Peronnas\n",
      "Away team:  Laval\n",
      "Features home:  tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.7000,  2.8800,  2.9000],\n",
      "        [-1.1730, -1.8492, -0.8932, -0.3694,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2500,  3.0000,  3.5000],\n",
      "        [-0.6845,  1.3282, -1.1527,  0.8992,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.4000,  3.0000,  3.2000],\n",
      "        [-0.2291,  1.8941, -1.4227,  0.5266,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  3.1000,  3.1000,  2.3800],\n",
      "        [ 0.2685,  1.2267, -1.1430, -0.6323,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.1000,  3.1000,  3.7500]])\n",
      "Features away:  tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2000,  3.0000,  3.6000],\n",
      "        [-0.3644, -0.6365, -0.8932, -1.1891,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.5000,  3.0000,  3.1000],\n",
      "        [-1.5235, -1.2494, -1.8152, -1.5288,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2000,  3.0000,  3.6000],\n",
      "        [-0.2285, -1.7327, -2.5471, -1.0356,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2500,  2.6300,  4.2000],\n",
      "        [-0.5538, -2.2725, -3.3109, -1.3215,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.1000,  3.1000,  3.7500]])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"First match\")\n",
    "print(\"Home team: \", idx_to_teams[training_home_teams_matches[index].item()])\n",
    "print(\"Away team: \", idx_to_teams[training_away_teams_matches[index].item()])\n",
    "print(\"Features home: \", training_matches_features_home[index])\n",
    "print(\"Features away: \", training_matches_features_away[index])\n",
    "# print(\"Target: \", idx_to_result[training_targets[index].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "19874\n",
      "Validation data\n",
      "4969\n",
      "Test data\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and validation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "split = 0.8\n",
    "split_idx = int(len(training_home_teams_matches) * split)\n",
    "\n",
    "train_dataset = TensorDataset(training_home_teams_matches[:split_idx], training_away_teams_matches[:split_idx], training_matches_features_home[:split_idx], training_matches_features_away[:split_idx], training_targets_oh[:split_idx])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # true could cause lookahead\n",
    "\n",
    "val_dataset = TensorDataset(training_home_teams_matches[split_idx:], training_away_teams_matches[split_idx:], training_matches_features_home[split_idx:], training_matches_features_away[split_idx:], training_targets_oh[split_idx:])\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(test_home_teams_matches, test_away_teams_matches, test_matches_features_home, test_matches_features_away, test_targets_oh)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(len(train_dataset))\n",
    "# print(len(train_loader))\n",
    "\n",
    "print(\"Validation data\")\n",
    "print(len(val_dataset))\n",
    "# print(len(val_loader))\n",
    "\n",
    "print(\"Test data\")\n",
    "print(len(test_dataset))\n",
    "# print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 50\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # true could cause lookahead\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for home_teams, away_teams, home_features, away_features, targets in train_loader:\n",
    "#     print(home_teams.shape)\n",
    "#     print(away_teams.shape)\n",
    "#     print(home_features.shape)\n",
    "#     print(away_features.shape)\n",
    "#     print(targets.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTM(\n",
    "            num_features=14,\n",
    "            inception_depth=2, \n",
    "            inception_out=24, \n",
    "            lstm_num_layers=2,\n",
    "            hidden_size=500, \n",
    "            num_heads=1, num_classes=3, bottleneck_dim = None)\n",
    "\n",
    "train_model(model_lstm, \n",
    "criterion=RPS_loss(),\n",
    "# criterion= nn.CrossEntropyLoss(),\n",
    "train_dataset=train_dataset, \n",
    "val_dataset=val_dataset,\n",
    "learning_rate=0.1,\n",
    "num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 Loss: 0.11096549034118652 Train Acc: 38.92964824120603 Val Acc: 36.52\n",
      "100 Loss: 0.10961967706680298 Train Acc: 41.29145728643216 Val Acc: 38.82\n",
      "150 Loss: 0.11596894264221191 Train Acc: 43.32663316582914 Val Acc: 43.16\n",
      "200 Loss: 0.11513856798410416 Train Acc: 43.41708542713568 Val Acc: 43.34\n",
      "250 Loss: 0.1091860681772232 Train Acc: 43.70854271356784 Val Acc: 43.78\n",
      "300 Loss: 0.10303571075201035 Train Acc: 43.49246231155779 Val Acc: 42.5\n"
     ]
    }
   ],
   "source": [
    "# in_channels = 14  # this should be the number of features in each data sample\n",
    "# # the remaining inputs are hyperparameters which can be tuned accordingly\n",
    "\n",
    "model_transformer = Transformer(\n",
    "            num_features=14,\n",
    "            inception_depth=2, \n",
    "            inception_out=24,\n",
    "            num_heads=8,\n",
    "            num_layers=6,\n",
    "            d_ff = 256,\n",
    "            d_h = 512,\n",
    "            max_timespan = 5, # axis (1) of data\n",
    "            dropout = 0.1)\n",
    "\n",
    "train_model(model_transformer, \n",
    "# criterion=RPS_loss(),\n",
    "criterion= nn.CrossEntropyLoss(),\n",
    "train_dataset=train_dataset, \n",
    "val_dataset=val_dataset,\n",
    "learning_rate=0.00001,\n",
    "num_epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
