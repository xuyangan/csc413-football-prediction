{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('../models/')\n",
    "from models.lstm import LSTM\n",
    "from models.transformer import Transformer\n",
    "from models.train_model import train_model, RPS_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_home_teams_matches = th.load(\"dataset/tensors/training_home_teams_matches.pt\")\n",
    "training_away_teams_matches = th.load(\"dataset/tensors/training_away_teams_matches.pt\")\n",
    "training_matches_features_home = th.load(\"dataset/tensors/training_matches_features_home.pt\")\n",
    "training_matches_features_away = th.load(\"dataset/tensors/training_matches_features_away.pt\")\n",
    "training_targets = th.load(\"dataset/tensors/training_targets.pt\")\n",
    "\n",
    "test_home_teams_matches = th.load(\"dataset/tensors/test_home_teams_matches.pt\")\n",
    "test_away_teams_matches = th.load(\"dataset/tensors/test_away_teams_matches.pt\")\n",
    "test_matches_features_home = th.load(\"dataset/tensors/test_matches_features_home.pt\")\n",
    "test_matches_features_away = th.load(\"dataset/tensors/test_matches_features_away.pt\")\n",
    "test_targets = th.load(\"dataset/tensors/test_targets.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H': 0, 'D': 1, 'A': 2}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "file1 = 'dataset/tensors/idx_to_teams.pkl'\n",
    "file2 = 'dataset/tensors/teams_to_idx.pkl'\n",
    "file3 = 'dataset/tensors/result_map.pkl'\n",
    "\n",
    "with open(file1, 'rb') as file:\n",
    "    idx_to_teams = pickle.load(file)\n",
    "\n",
    "with open(file2, 'rb') as file:\n",
    "    teams_to_idx = pickle.load(file)\n",
    "\n",
    "with open(file3, 'rb') as file:\n",
    "    result_map = pickle.load(file)\n",
    "\n",
    "print(result_map)\n",
    "idx_to_result = {0: 'H', 1: 'D', 2: 'A'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "torch.Size([24843])\n",
      "torch.Size([24843])\n",
      "torch.Size([24843, 5, 14])\n",
      "torch.Size([24843, 5, 14])\n",
      "torch.Size([24843])\n",
      "Test data\n",
      "torch.Size([3072])\n",
      "torch.Size([3072])\n",
      "torch.Size([3072, 5, 14])\n",
      "torch.Size([3072, 5, 14])\n",
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\")\n",
    "print(training_home_teams_matches.shape) # the idex of home team for the matches\n",
    "print(training_away_teams_matches.shape) # the idex of away team for the matches\n",
    "print(training_matches_features_home.shape) # the features of the home team for the matches\n",
    "print(training_matches_features_away.shape) # the features of the away team for the matches\n",
    "print(training_targets.shape) # the targets of the matches H D A\n",
    "\n",
    "print(\"Test data\")\n",
    "print(test_home_teams_matches.shape)\n",
    "print(test_away_teams_matches.shape)\n",
    "print(test_matches_features_home.shape)\n",
    "print(test_matches_features_away.shape)\n",
    "print(test_targets.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24843, 3])\n",
      "torch.Size([3072, 3])\n",
      "tensor([0, 1, 0])\n",
      "tensor([0, 0, 1])\n",
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([0, 0, 1])\n",
      "tensor([1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def one_hot_targets(targets):\n",
    "    num_classes = th.max(targets).item() + 1\n",
    "    return F.one_hot(targets, num_classes=num_classes)\n",
    "\n",
    "training_targets = one_hot_targets(training_targets)\n",
    "test_targets = one_hot_targets(test_targets)\n",
    "\n",
    "print(training_targets.shape)\n",
    "print(test_targets.shape)\n",
    "\n",
    "for i in range(3):\n",
    "    print(training_targets[i])\n",
    "    print(test_targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First match\n",
      "Home team:  Bourg Peronnas\n",
      "Away team:  Laval\n",
      "Features home:  tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.7000,  2.8800,  2.9000],\n",
      "        [-1.1730, -1.8492, -0.8932, -0.3694,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2500,  3.0000,  3.5000],\n",
      "        [-0.6845,  1.3282, -1.1527,  0.8992,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.4000,  3.0000,  3.2000],\n",
      "        [-0.2291,  1.8941, -1.4227,  0.5266,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  3.1000,  3.1000,  2.3800],\n",
      "        [ 0.2685,  1.2267, -1.1430, -0.6323,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.1000,  3.1000,  3.7500]])\n",
      "Features away:  tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2000,  3.0000,  3.6000],\n",
      "        [-0.3644, -0.6365, -0.8932, -1.1891,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.5000,  3.0000,  3.1000],\n",
      "        [-1.5235, -1.2494, -1.8152, -1.5288,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2000,  3.0000,  3.6000],\n",
      "        [-0.2285, -1.7327, -2.5471, -1.0356,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.2500,  2.6300,  4.2000],\n",
      "        [-0.5538, -2.2725, -3.3109, -1.3215,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  2.1000,  3.1000,  3.7500]])\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"First match\")\n",
    "print(\"Home team: \", idx_to_teams[training_home_teams_matches[index].item()])\n",
    "print(\"Away team: \", idx_to_teams[training_away_teams_matches[index].item()])\n",
    "print(\"Features home: \", training_matches_features_home[index])\n",
    "print(\"Features away: \", training_matches_features_away[index])\n",
    "# print(\"Target: \", idx_to_result[training_targets[index].item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "19874\n",
      "Validation data\n",
      "4969\n",
      "Test data\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "#split the data into training and validation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "split = 0.8\n",
    "split_idx = int(len(training_home_teams_matches) * split)\n",
    "\n",
    "train_dataset = TensorDataset(training_home_teams_matches[:split_idx], training_away_teams_matches[:split_idx], training_matches_features_home[:split_idx], training_matches_features_away[:split_idx], training_targets[:split_idx])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) # true could cause lookahead\n",
    "\n",
    "val_dataset = TensorDataset(training_home_teams_matches[split_idx:], training_away_teams_matches[split_idx:], training_matches_features_home[split_idx:], training_matches_features_away[split_idx:], training_targets[split_idx:])\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_dataset = TensorDataset(test_home_teams_matches, test_away_teams_matches, test_matches_features_home, test_matches_features_away, test_targets)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(len(train_dataset))\n",
    "# print(len(train_loader))\n",
    "\n",
    "print(\"Validation data\")\n",
    "print(len(val_dataset))\n",
    "# print(len(val_loader))\n",
    "\n",
    "print(\"Test data\")\n",
    "print(len(test_dataset))\n",
    "# print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False) # true could cause lookahead\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 5, 14])\n",
      "torch.Size([100, 5, 14])\n",
      "torch.Size([100, 3])\n"
     ]
    }
   ],
   "source": [
    "for home_teams, away_teams, home_features, away_features, targets in train_loader:\n",
    "    print(home_teams.shape)\n",
    "    print(away_teams.shape)\n",
    "    print(home_features.shape)\n",
    "    print(away_features.shape)\n",
    "    print(targets.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = LSTM(\n",
    "            num_features=14,\n",
    "            inception_depth=3, \n",
    "            inception_out=256, \n",
    "            hidden_size=16, \n",
    "            num_heads=4, num_classes=3, bottleneck_dim = None)\n",
    "\n",
    "train_model(model_lstm, \n",
    "criterion=RPS_loss,\n",
    "# criterion= nn.CrossEntropyLoss(ignore_index=0),\n",
    "train_loader=train_loader, \n",
    "val_loader=val_loader,\n",
    "learning_rate=0.1,\n",
    "num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LSTM.__init__() got an unexpected keyword argument 'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# in_channels = 14  # this should be the number of features in each data sample\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# # the remaining inputs are hyperparameters which can be tuned accordingly\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43minception_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43minception_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43md_h\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_timespan\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m train_model(model_transformer, \n\u001b[1;32m     17\u001b[0m criterion\u001b[38;5;241m=\u001b[39mRPS_loss,\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# criterion= nn.CrossEntropyLoss(ignore_index=0),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     22\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: LSTM.__init__() got an unexpected keyword argument 'num_layers'"
     ]
    }
   ],
   "source": [
    "# in_channels = 14  # this should be the number of features in each data sample\n",
    "# # the remaining inputs are hyperparameters which can be tuned accordingly\n",
    "\n",
    "\n",
    "model_transformer = LSTM(\n",
    "            num_features=14,\n",
    "            inception_depth=3,\n",
    "            inception_out=256,\n",
    "            num_heads=8,\n",
    "            num_layers=6,\n",
    "            d_ff = 256,\n",
    "            d_h = 512,\n",
    "            max_timespan = 50,\n",
    "            dropout = 0.1,)\n",
    "\n",
    "train_model(model_transformer, \n",
    "criterion=RPS_loss,\n",
    "# criterion= nn.CrossEntropyLoss(ignore_index=0),\n",
    "train_loader=train_loader, \n",
    "val_loader=val_loader,\n",
    "learning_rate=0.1,\n",
    "num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
